{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "780a1904-2987-455f-90ef-5de906bf8863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.functions import col, expr, sqrt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import t, f\n",
    "import matplotlib.pyplot as plt  # Diesen Import hinzufÃ¼gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d0d7bb0-bb83-4dd1-bc10-c99a76b7af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.master(\"spark://spark-master:7077\").appName(\"Test\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15e67860-c100-450b-a82a-8147fc35694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backward_substitution(A_dict, y):\n",
    "    n = len(A_dict)\n",
    "    b = np.zeros(n)\n",
    "\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        row = A_dict.get(i)\n",
    "        if row is None:\n",
    "            raise ValueError(f\"No row found in A_rdd for index {i}\")\n",
    "        if row[i] == 0:\n",
    "            raise ValueError(\"Matrix A contains a zero on the diagonal; no unique solution possible.\")\n",
    "        \n",
    "        b[i] = (y[i] - np.dot(row[i+1:], b[i+1:])) / row[i]\n",
    "\n",
    "    return b\n",
    "\n",
    "def gram_schmidt(X_df):\n",
    "    X_rdd = X_df.rdd.cache()\n",
    "    m = len(X_df.first().features)\n",
    "    Q = []\n",
    "    R = np.zeros((m, m))\n",
    "\n",
    "    for j in range(m):\n",
    "        v = X_rdd.map(lambda row: row.features[j]).collect()\n",
    "        \n",
    "        for i in range(j):\n",
    "            R[i, j] = np.dot(Q[i], v)\n",
    "            v -= R[i, j] * Q[i]\n",
    "\n",
    "        R[j, j] = np.linalg.norm(v)\n",
    "        Q.append(v / R[j, j])\n",
    "    \n",
    "    R_dict = {i: R[i, :] for i in range(m)}\n",
    "\n",
    "    Q_df = spark.createDataFrame([Row(features=DenseVector(q)) for q in zip(*Q)])\n",
    "\n",
    "    return Q_df, R_dict\n",
    "\n",
    "def create_data(n, p, beta_true):\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(n, p)\n",
    "    X = np.column_stack([np.ones(X.shape[0]), X])\n",
    "    y = X @ beta_true + np.random.randn(n) * 0.1\n",
    "    data = [Row(y=float(y_i), **{f\"x{i}\": float(x_i) for i, x_i in enumerate(x)}) for x, y_i in zip(X, y)]\n",
    "    df = spark.createDataFrame(data).repartition(100)\n",
    "    feature_columns = [f\"x{i}\" for i in range(p + 1)]  # Include intercept column\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    return assembler.transform(df).select(\"features\", \"y\").repartition(100)\n",
    "\n",
    "def linear_regression_manual_qr(X, y):\n",
    "    start_time = time.time()\n",
    "    Q, R = gram_schmidt(X)\n",
    "    Qt_y = np.array(Q.rdd.map(lambda row: sum(row.features[i] * y[i] for i in range(len(row.features)))).collect())\n",
    "    beta = backward_substitution(R, Qt_y)\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    X_mat = np.array(X.rdd.map(lambda row: row.features).collect())  # Collect the features matrix as a numpy array\n",
    "    n, k = X_mat.shape\n",
    "    k -= 1  # -1 to account for the intercept column\n",
    "\n",
    "    df_residuals = n - k - 1  # Degrees of freedom\n",
    "    XtX = np.dot(X_mat.T, X_mat)\n",
    "    XtX_inv = np.linalg.inv(XtX)\n",
    "\n",
    "    # Predicted y values\n",
    "    y_pred = np.dot(X_mat, beta)\n",
    "    residuals = y - y_pred\n",
    "\n",
    "    # R-squared and adjusted R-squared\n",
    "    r_quadrat = 1 - (np.sum(residuals ** 2) / np.sum((y - np.mean(y)) ** 2))\n",
    "    adjusted_r_quadrat = 1 - ((n - 1) / df_residuals) * (np.sum(residuals ** 2) / np.sum((y - np.mean(y)) ** 2))\n",
    "\n",
    "    # Sum of Squared Errors (SSE), Sum of Squared Regression (SSR)\n",
    "    SSE = np.sum(residuals ** 2)\n",
    "    SSR = np.sum((y_pred - np.mean(y)) ** 2)\n",
    "    sigma_hat = np.sqrt(SSE / df_residuals)\n",
    "\n",
    "    # F-statistics\n",
    "    f_statistics = (SSR / k) / (SSE / df_residuals)\n",
    "    f_p_wert = stats.f.sf(f_statistics, k, df_residuals)\n",
    "\n",
    "    # Log-likelihood, AIC, BIC\n",
    "    log_likelihood = -n / 2 * (np.log(2 * np.pi) + np.log(sigma_hat ** 2) + SSE / (n * sigma_hat ** 2))\n",
    "    AIC = 2 * k - 2 * log_likelihood\n",
    "    BIC = np.log(n) * k - 2 * log_likelihood\n",
    "\n",
    "    # Covariance matrix and standard errors\n",
    "    cov_matrix = (sigma_hat ** 2) * XtX_inv\n",
    "    se = np.sqrt(np.diag(cov_matrix))\n",
    "    t_werte = beta / se\n",
    "    p_werte = 2 * (1 - stats.t.cdf(np.abs(t_werte), df_residuals))\n",
    "\n",
    "    # Confidence intervals\n",
    "    alpha = 0.05\n",
    "    t_crit = stats.t.ppf(1 - alpha / 2, df=df_residuals)\n",
    "    conf_int_lower = beta - t_crit * se\n",
    "    conf_int_upper = beta + t_crit * se\n",
    "\n",
    "    # Omnibus test, Durbin-Watson statistic, Jarque-Bera test\n",
    "    skewness = stats.skew(residuals)\n",
    "    kurtosis = stats.kurtosis(residuals, fisher=False)\n",
    "    omnibus_stat = (n / 6) * (skewness**2 + ((kurtosis - 3)**2) / 4)\n",
    "    prob_omnibus = 1 - stats.chi2.cdf(omnibus_stat, df=2)\n",
    "    dw_statistic = np.sum(np.diff(residuals) ** 2) / np.sum(residuals ** 2)\n",
    "    jarque_bera_stat = (n / 6) * (skewness**2 + (kurtosis - 3)**2 / 4)\n",
    "    prob_jb = 1 - stats.chi2.cdf(jarque_bera_stat, df=2)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    result = [\n",
    "        {'Dep. Variable': 'y',\n",
    "         'Method': 'Linear Regression with QR Decomposition',\n",
    "         'Observations': n,\n",
    "         'df': df_residuals,\n",
    "         'Variables': k},\n",
    "        {'R-Squared': r_quadrat,\n",
    "         'Adj. R-Squared': adjusted_r_quadrat,\n",
    "         'F-statistic': f_statistics,\n",
    "         'Prob (F-statistic)': f_p_wert,\n",
    "         'Log-Likelihood': log_likelihood,\n",
    "         'AIC': AIC,\n",
    "         'BIC': BIC},\n",
    "        {'coef': beta,\n",
    "         'std err': se,\n",
    "         't': t_werte,\n",
    "         'P>abs(t)': p_werte,\n",
    "         'lower boundary': conf_int_lower,\n",
    "         'upper boundary': conf_int_upper},\n",
    "        {'Ombibus': omnibus_stat,\n",
    "         'Prob(Omnibus)': prob_omnibus,\n",
    "         'Skew': skewness,\n",
    "         'Kurtosis': kurtosis,\n",
    "         'Durbin-Watson': dw_statistic,\n",
    "         'Jarque-Bera (JB)': jarque_bera_stat,\n",
    "         'Prob(JB)': prob_jb}\n",
    "    ]\n",
    "\n",
    "    return result, elapsed_time\n",
    "\n",
    "def run_benchmark(n_list, repetitions=5):\n",
    "    results = []\n",
    "    beta_true = [-8, -1.6, 4.1, -10, -9.2, 1.3, 1.6, 2.3]\n",
    "    p = 7\n",
    "\n",
    "    for n in n_list:\n",
    "        times = []\n",
    "        for _ in range(repetitions):\n",
    "            X_y_df = create_data(n, p, beta_true)\n",
    "            X = X_y_df.select(\"features\")\n",
    "            y = X_y_df.select(\"y\").rdd.flatMap(lambda x: x).collect()\n",
    "            result, elapsed_time = linear_regression_manual_qr(X, y)\n",
    "            times.append(elapsed_time)\n",
    "\n",
    "        avg_time = sum(times) / repetitions\n",
    "        std_time = (sum((x - avg_time) ** 2 for x in times) / repetitions) ** 0.5\n",
    "        results.append([n, avg_time, std_time, result])\n",
    "        print(\"\\nData Rows:\", n)\n",
    "        print(\"Average Run Time:\", avg_time)\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca3874e2-0a3d-4276-a57c-db966351ea4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Rows: 200000\n",
      "Average Run Time: 30.760185050964356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Rows: 500000\n",
      "Average Run Time: 43.66273641586304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Rows: 1000000\n",
      "Average Run Time: 68.14592761993408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/18 13:20:24 ERROR Inbox: An error happened while processing message in the inbox for CoarseGrainedScheduler\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.util.Arrays.copyOf(Arrays.java:3236)\n",
      "\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n",
      "\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n",
      "\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n",
      "\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n",
      "\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n",
      "\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:530)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:494)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager$$Lambda$2420/1858157486.apply(Unknown Source)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:470)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:414)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:409)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$2411/372025867.apply(Unknown Source)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:409)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$2407/905979935.apply$mcVI$sp(Unknown Source)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:399)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:606)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:601)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$2406/1543942411.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:601)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:574)\n",
      "Exception in thread \"dispatcher-CoarseGrainedScheduler\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.util.Arrays.copyOf(Arrays.java:3236)\n",
      "\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n",
      "\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n",
      "\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n",
      "\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n",
      "\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n",
      "\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:530)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:494)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager$$Lambda$2420/1858157486.apply(Unknown Source)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:470)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:414)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:409)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$2411/372025867.apply(Unknown Source)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:409)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$2407/905979935.apply$mcVI$sp(Unknown Source)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:399)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:606)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:601)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$2406/1543942411.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:601)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:574)\n",
      "ERROR:root:KeyboardInterrupt while sending command.              (20 + 27) / 48]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/local/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m n_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m200000\u001b[39m, \u001b[38;5;241m500000\u001b[39m, \u001b[38;5;241m1000000\u001b[39m, \u001b[38;5;241m5000000\u001b[39m, \u001b[38;5;241m10000000\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m benchmark_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 150\u001b[0m, in \u001b[0;36mrun_benchmark\u001b[0;34m(n_list, repetitions)\u001b[0m\n\u001b[1;32m    148\u001b[0m X_y_df \u001b[38;5;241m=\u001b[39m create_data(n, p, beta_true)\n\u001b[1;32m    149\u001b[0m X \u001b[38;5;241m=\u001b[39m X_y_df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 150\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mX_y_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241m.\u001b[39mflatMap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x)\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m    151\u001b[0m result, elapsed_time \u001b[38;5;241m=\u001b[39m linear_regression_manual_qr(X, y)\n\u001b[1;32m    152\u001b[0m times\u001b[38;5;241m.\u001b[39mappend(elapsed_time)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:214\u001b[0m, in \u001b[0;36mDataFrame.rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m<class 'pyspark.rdd.RDD'>\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 214\u001b[0m     jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjavaToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;241m=\u001b[39m RDD(\n\u001b[1;32m    216\u001b[0m         jrdd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39m_sc, BatchedSerializer(CPickleSerializer())\n\u001b[1;32m    217\u001b[0m     )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 640:======================>                               (20 + 27) / 48]\r"
     ]
    }
   ],
   "source": [
    "n_values = [200000, 500000, 1000000, 5000000, 10000000]\n",
    "benchmark_results = run_benchmark(n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e1f5e-2563-48cc-afc7-fe9b05c7c648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c08178-a41a-40f8-8412-3ae2566974ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 330:==================>                                   (16 + 31) / 48]\r"
     ]
    }
   ],
   "source": [
    "n_values = [5000000, 10000000]\n",
    "benchmark_results = run_benchmark(n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c1548-7b21-4e9d-ade7-bac0a87e0bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
