{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d18953f-da62-41e0-ab13-13701ddc465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.functions import col, expr, sqrt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5028df0a-be45-4550-b2d1-43ed6579b404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/18 10:55:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder.master(\"spark://spark-master:7077\").appName(\"Test\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "302f4298-df77-4e1f-9a09-fbab826e85fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "780a1904-2987-455f-90ef-5de906bf8863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Rows: 200000\n",
      "Average Run Time: 14.521358060836793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Rows: 500000\n",
      "Average Run Time: 33.109078073501585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Rows: 1000000\n",
      "Average Run Time: 110.83982186317444\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def backward_substitution(A_dict, y):\n",
    "    n = len(A_dict)\n",
    "    b = np.zeros(n)\n",
    "\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        row = A_dict.get(i)\n",
    "        if row is None:\n",
    "            raise ValueError(f\"No row found in A_rdd for index {i}\")\n",
    "        if row[i] == 0:\n",
    "            raise ValueError(\"Matrix A contains a zero on the diagonal; no unique solution possible.\")\n",
    "        \n",
    "        b[i] = (y[i] - np.dot(row[i+1:], b[i+1:])) / row[i]\n",
    "\n",
    "    return b\n",
    "\n",
    "def gram_schmidt(X_df):\n",
    "    X_rdd = X_df.rdd.cache()\n",
    "    m = len(X_df.first().features)\n",
    "    Q = []\n",
    "    R = np.zeros((m, m))\n",
    "\n",
    "    for j in range(m):\n",
    "        v = X_rdd.map(lambda row: row.features[j]).collect()\n",
    "        \n",
    "        for i in range(j):\n",
    "            R[i, j] = np.dot(Q[i], v)\n",
    "            v -= R[i, j] * Q[i]\n",
    "\n",
    "        R[j, j] = np.linalg.norm(v)\n",
    "        Q.append(v / R[j, j])\n",
    "    \n",
    "    R_dict = {i: R[i, :] for i in range(m)}\n",
    "\n",
    "    Q_df = spark.createDataFrame([Row(features=DenseVector(q)) for q in zip(*Q)])\n",
    "\n",
    "    return Q_df, R_dict\n",
    "\n",
    "def create_data(n, p, beta_true):\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(n, p)\n",
    "    X = np.column_stack([np.ones(X.shape[0]), X])\n",
    "    y = X @ beta_true + np.random.randn(n) * 0.1\n",
    "    data = [Row(y=float(y_i), **{f\"x{i}\": float(x_i) for i, x_i in enumerate(x)}) for x, y_i in zip(X, y)]\n",
    "    df = spark.createDataFrame(data)\n",
    "    feature_columns = [f\"x{i}\" for i in range(p + 1)]  # Include intercept column\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    return assembler.transform(df).select(\"features\", \"y\")\n",
    "\n",
    "def linear_regression_manual_qr(X, y):\n",
    "    start_time = time.time()\n",
    "    Q, R = gram_schmidt(X)\n",
    "    Qt_y = np.array(Q.rdd.map(lambda row: sum(row.features[i] * y[i] for i in range(len(row.features)))).collect())\n",
    "    beta = backward_substitution(R, Qt_y)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    return beta, elapsed_time\n",
    "\n",
    "def run_benchmark(n_list, repetitions=5):\n",
    "    results = []\n",
    "    beta_true = [-8, -1.6, 4.1, -10, -9.2, 1.3, 1.6, 2.3]\n",
    "    p = 7\n",
    "\n",
    "    for n in n_list:\n",
    "        times = []\n",
    "        for _ in range(repetitions):\n",
    "            X_y_df = create_data(n, p, beta_true)\n",
    "            X = X_y_df.select(\"features\")\n",
    "            y = X_y_df.select(\"y\").rdd.flatMap(lambda x: x).collect()\n",
    "            beta, elapsed_time = linear_regression_manual_qr(X, y)\n",
    "            times.append(elapsed_time)\n",
    "\n",
    "        avg_time = sum(times) / repetitions\n",
    "        std_time = (sum((x - avg_time) ** 2 for x in times) / repetitions) ** 0.5\n",
    "        results.append([n, avg_time, std_time, beta])\n",
    "        print(\"\\nData Rows:\", n)\n",
    "        print(\"Average Run Time:\", avg_time)\n",
    "\n",
    "    return results\n",
    "\n",
    "n_values = [200000, 500000, 1000000]\n",
    "benchmark_results = run_benchmark(n_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c1548-7b21-4e9d-ade7-bac0a87e0bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225866cf-3021-4a34-ba34-4bdc5109e336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
