{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b96fd54-fe71-4266-975e-a9962c46d002",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"spark://spark-master:7077\").appName(\"Test\").getOrCreate()\n",
    "spark.sparkContext.addPyFile('numpy.zip')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "745a18a7-b2f5-474b-808d-90473f4e7141",
   "metadata": {},
   "source": [
    "import requests\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "url = 'https://drive.usercontent.google.com/download?id=1z_AfT9UYwG7XqExz95MZOr_Ix4jDUXFw&export=download'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "csv_data = response.text\n",
    "csv_reader = csv.reader(StringIO(csv_data))\n",
    "\n",
    "header = next(csv_reader)\n",
    "rows = [tuple(row) for row in csv_reader]\n",
    "\n",
    "# Read the CSV using PySpark\n",
    "df = spark.createDataFrame(rows, schema=header)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be436538-535d-435f-9bfa-2ba31f5fc617",
   "metadata": {},
   "source": [
    "from scipy.stats import t, f\n",
    "from pyspark.ml.linalg import DenseMatrix, DenseVector\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "\n",
    "def matrix_outer(x):\n",
    "    return [[x_i * x_j for x_j in x] for x_i in x]\n",
    "\n",
    "X_df = df.withColumn(\"intercept\", F.lit(1))\n",
    "\n",
    "n = df.count()\n",
    "k = len(df.columns) - 1\n",
    "\n",
    "X_rdd = df.select(df.columns[1:]).rdd.map(lambda row: [1.0] + [float(x) for x in row])\n",
    "Y_rdd = df.select(df.columns[:1]).rdd.map(lambda row: float(row[0]))\n",
    "\n",
    "XtX = X_rdd.map(lambda x: matrix_outer(x)).reduce(\n",
    "    lambda a, b: [[a[i][j] + b[i][j] for j in range(len(a[0]))] for i in range(len(a))]\n",
    ")\n",
    "XtY = X_rdd.zip(Y_rdd).map(lambda x: [x_i * x[1] for x_i in x[0]]).reduce(lambda a, b: [a_i + b_i for a_i, b_i in zip(a, b)])\n",
    "XtX_inv = np.linalg.inv(XtX)\n",
    "beta = [sum(XtX_inv[i][j] * XtY[j] for j in range(len(XtY))) for i in range(len(XtX_inv))]\n",
    "\n",
    "beta_broadcast = spark.sparkContext.broadcast(beta)\n",
    "\n",
    "Y_pred_rdd = X_rdd.map(lambda x: sum(beta_broadcast.value[i] * x[i] for i in range(len(x))))\n",
    "residuals_rdd = Y_rdd.zip(Y_pred_rdd).map(lambda x: float(x[0]) - float(x[1]))\n",
    "f_residuals = n - k - 1\n",
    "Y_mean = Y_rdd.reduce(lambda a, b: a + b) / n\n",
    "SSE = residuals_rdd.map(lambda res: res ** 2).sum()\n",
    "#SST = Y_rdd.map(lambda y: (y - Y_mean) ** 2).reduce(lambda a, b: a + b)\n",
    "\n",
    "#sigma_hat = np.sqrt(SSE / f_residuals)\n",
    "#cov_matrix = [[sigma_hat ** 2 * XtX_inv[i][j] for j in range(len(XtX_inv[0]))] for i in range(len(XtX_inv))]\n",
    "\n",
    "Y_mean"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c84ae594-da5e-41a6-aff8-4b17e4dba267",
   "metadata": {},
   "source": [
    "from scipy.stats import t, f\n",
    "from pyspark.ml.linalg import DenseMatrix, DenseVector\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "\n",
    "n = df.count()\n",
    "k = len(df.columns) - 1\n",
    "\n",
    "X = np.array(df.select(df.columns[1:]).rdd.map(lambda row: [1] + list(row)).collect(), dtype=float)\n",
    "Y = np.array(df.select(df.columns[:1]).rdd.map(lambda row: row[0]).collect(), dtype=float)\n",
    "\n",
    "\n",
    "XtX = np.dot(X.T, X)\n",
    "XtY = np.dot(X.T, Y)\n",
    "XtX_inv = np.linalg.inv(XtX)\n",
    "\n",
    "beta = np.dot(XtX_inv, XtY)\n",
    "\n",
    "Y_pred = np.dot(X, beta)\n",
    "\n",
    "residuals = Y - Y_pred\n",
    "f_residuals = n - k - 1\n",
    "\n",
    "SSE = np.sum(residuals ** 2)\n",
    "SSR = np.sum((beta - np.mean(Y)) ** 2)\n",
    "SST = SSR + SSE\n",
    "\n",
    "sigma_hat = np.sqrt(SSE / f_residuals)\n",
    "cov_matrix = (sigma_hat ** 2) * XtX_inv\n",
    "se = np.sqrt(np.diag(cov_matrix))\n",
    "t_werte = beta / se\n",
    "p_werte = 2 * (1 - t.cdf(np.abs(t_werte), f_residuals))\n",
    "f_statistics = (SSR / k) / (SSE / f_residuals)\n",
    "f_p_wert = 1 - f.cdf(f_statistics, k, f_residuals)\n",
    "quantile1_3 = np.quantile(residuals, (0.25, 0.75))\n",
    "quantile = [np.min(residuals), quantile1_3[0], np.median(residuals), quantile1_3[1], np.max(residuals)]\n",
    "r_quadrat = 1 - (np.sum((Y - Y_pred) ** 2) / np.sum((Y - np.mean(Y)) ** 2))\n",
    "adjusted_r_quadrat = 1 - ((n - 1) / f_residuals) * (np.sum((Y - Y_pred) ** 2) / np.sum((Y - np.mean(Y)) ** 2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75edbea9-c9c5-4822-afd4-6b44a501603c",
   "metadata": {},
   "source": [
    "spark.stop()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd34f1d2-1036-4ea0-b659-b5f4d1ed7ec4",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array(df.select(df.columns[1:]).rdd.map(lambda row: [1] + list(row)).collect(), dtype=float)\n",
    "XtX = np.dot(X.T, X)\n",
    "XtX"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ff91a-bfa6-4e13-8a43-854eee41990a",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
