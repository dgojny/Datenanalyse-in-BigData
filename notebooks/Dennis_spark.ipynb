{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b96fd54-fe71-4266-975e-a9962c46d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"spark://spark-master:7077\").appName(\"Test\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "745a18a7-b2f5-474b-808d-90473f4e7141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+\n",
      "|  Y| X1| X2| X3| X4| X5| X6|\n",
      "+---+---+---+---+---+---+---+\n",
      "| 43| 51| 30| 39| 61| 92|45 |\n",
      "| 63| 64| 51| 54| 63| 73|47 |\n",
      "| 71| 70| 68| 69| 76| 86|48 |\n",
      "| 61| 63| 45| 47| 54| 84|35 |\n",
      "| 81| 78| 56| 66| 71| 83|47 |\n",
      "| 43| 55| 49| 44| 54| 49|34 |\n",
      "| 58| 67| 42| 56| 66| 68|35 |\n",
      "| 71| 75| 50| 55| 70| 66|41 |\n",
      "| 72| 82| 72| 67| 71| 83|31 |\n",
      "| 67| 61| 45| 47| 62| 80|41 |\n",
      "| 64| 53| 53| 58| 58| 67|34 |\n",
      "| 67| 60| 47| 39| 59| 74|41 |\n",
      "| 69| 62| 57| 42| 55| 63|25 |\n",
      "| 68| 83| 83| 45| 59| 77|35 |\n",
      "| 77| 77| 54| 72| 79| 77|46 |\n",
      "| 81| 90| 50| 72| 60| 54|36 |\n",
      "| 74| 85| 64| 69| 79| 79|63 |\n",
      "| 65| 60| 65| 75| 55| 80|60 |\n",
      "| 65| 70| 46| 57| 75| 85|46 |\n",
      "| 50| 58| 68| 54| 64| 78|52 |\n",
      "+---+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "url = 'https://drive.usercontent.google.com/download?id=1z_AfT9UYwG7XqExz95MZOr_Ix4jDUXFw&export=download'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "csv_data = response.text\n",
    "csv_reader = csv.reader(StringIO(csv_data))\n",
    "\n",
    "header = next(csv_reader)\n",
    "rows = [tuple(row) for row in csv_reader]\n",
    "\n",
    "# Read the CSV using PySpark\n",
    "df = spark.createDataFrame(rows, schema=header)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be436538-535d-435f-9bfa-2ba31f5fc617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 51.0, 30.0, 39.0, 61.0, 92.0, 45.0], [1.0, 64.0, 51.0, 54.0, 63.0, 73.0, 47.0], [1.0, 70.0, 68.0, 69.0, 76.0, 86.0, 48.0], [1.0, 63.0, 45.0, 47.0, 54.0, 84.0, 35.0], [1.0, 78.0, 56.0, 66.0, 71.0, 83.0, 47.0], [1.0, 55.0, 49.0, 44.0, 54.0, 49.0, 34.0], [1.0, 67.0, 42.0, 56.0, 66.0, 68.0, 35.0], [1.0, 75.0, 50.0, 55.0, 70.0, 66.0, 41.0], [1.0, 82.0, 72.0, 67.0, 71.0, 83.0, 31.0], [1.0, 61.0, 45.0, 47.0, 62.0, 80.0, 41.0], [1.0, 53.0, 53.0, 58.0, 58.0, 67.0, 34.0], [1.0, 60.0, 47.0, 39.0, 59.0, 74.0, 41.0], [1.0, 62.0, 57.0, 42.0, 55.0, 63.0, 25.0], [1.0, 83.0, 83.0, 45.0, 59.0, 77.0, 35.0], [1.0, 77.0, 54.0, 72.0, 79.0, 77.0, 46.0], [1.0, 90.0, 50.0, 72.0, 60.0, 54.0, 36.0], [1.0, 85.0, 64.0, 69.0, 79.0, 79.0, 63.0], [1.0, 60.0, 65.0, 75.0, 55.0, 80.0, 60.0], [1.0, 70.0, 46.0, 57.0, 75.0, 85.0, 46.0], [1.0, 58.0, 68.0, 54.0, 64.0, 78.0, 52.0], [1.0, 40.0, 33.0, 34.0, 43.0, 64.0, 33.0], [1.0, 61.0, 52.0, 62.0, 66.0, 80.0, 41.0], [1.0, 66.0, 52.0, 50.0, 63.0, 80.0, 37.0], [1.0, 37.0, 42.0, 58.0, 50.0, 57.0, 49.0], [1.0, 54.0, 42.0, 48.0, 66.0, 75.0, 33.0], [1.0, 77.0, 66.0, 63.0, 88.0, 76.0, 72.0], [1.0, 75.0, 58.0, 74.0, 80.0, 78.0, 49.0], [1.0, 57.0, 44.0, 45.0, 51.0, 83.0, 38.0], [1.0, 85.0, 71.0, 71.0, 77.0, 74.0, 55.0], [1.0, 82.0, 39.0, 59.0, 64.0, 78.0, 39.0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t, f\n",
    "from pyspark.ml.linalg import DenseMatrix, DenseVector\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "\n",
    "def matrix_outer(x):\n",
    "    return [[x_i * x_j for x_j in x] for x_i in x]\n",
    "\n",
    "n = df.count()\n",
    "k = len(df.columns) - 1\n",
    "\n",
    "X_rdd = df.select(df.columns[1:]).rdd.map(lambda row: [1.0] + [float(x) for x in row])\n",
    "Y_rdd = df.select(df.columns[:1]).rdd.map(lambda row: float(row[0]))\n",
    "print(X_rdd.collect())\n",
    "\n",
    "XtX = X_rdd.map(lambda x: matrix_outer(x)).reduce(\n",
    "    lambda a, b: [[a[i][j] + b[i][j] for j in range(len(a[0]))] for i in range(len(a))]\n",
    ")\n",
    "XtY = X_rdd.zip(Y_rdd).map(lambda x: [x_i * x[1] for x_i in x[0]]).reduce(lambda a, b: [a_i + b_i for a_i, b_i in zip(a, b)])\n",
    "XtX_inv = np.linalg.inv(XtX)\n",
    "beta = [sum(XtX_inv[i][j] * XtY[j] for j in range(len(XtY))) for i in range(len(XtX_inv))]\n",
    "\n",
    "beta_broadcast = spark.sparkContext.broadcast(beta)\n",
    "\n",
    "Y_pred_rdd = X_rdd.map(lambda x: sum(beta_broadcast.value[i] * x[i] for i in range(len(x))))\n",
    "residuals_rdd = Y_rdd.zip(Y_pred_rdd).map(lambda x: float(x[0]) - float(x[1]))\n",
    "f_residuals = n - k - 1\n",
    "\n",
    "Y_mean = Y_rdd.reduce(lambda a, b: a + b) / n\n",
    "Y_mean_broadcast = spark.sparkContext.broadcast(Y_mean)\n",
    "\n",
    "SSE = residuals_rdd.map(lambda res: res ** 2).sum()\n",
    "SST = Y_rdd.map(lambda y: (y - Y_mean_broadcast.value) ** 2).sum()\n",
    "\n",
    "sigma_hat = np.sqrt(SSE / f_residuals)\n",
    "cov_matrix = sigma_hat ** 2 * XtX_inv\n",
    "\n",
    "se = np.sqrt(np.diag(cov_matrix))\n",
    "se_broadcast = spark.sparkContext.broadcast(se)\n",
    "\n",
    "t_values_rdd = spark.sparkContext.parallelize(range(len(beta))).map(\n",
    "    lambda i: beta_broadcast.value[i] / se_broadcast.value[i]\n",
    ")\n",
    "p_values_rdd = t_values_rdd.map(\n",
    "    lambda t_value: 2 * (1 - t.cdf(np.abs(t_value), df=n - k - 1))\n",
    ")\n",
    "\n",
    "p_values = p_values_rdd.collect()\n",
    "\n",
    "SSR = np.sum((beta - np.mean(Y_rdd.collect())) ** 2)\n",
    "f_statistics = (SSR / k) / (SSE / f_residuals)\n",
    "f_p_value = 1 - f.cdf(f_statistics, k, f_residuals)\n",
    "\n",
    "r_squared = 1 - (SSE / SST)\n",
    "adjusted_r_squared = 1 - (((n - 1) / f_residuals) * (SSE / SST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c84ae594-da5e-41a6-aff8-4b17e4dba267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'Coefficients':    Estimate  Std_Error  t_value   p_value\n",
       "  0  10.78708   11.58926    0.931  0.361634\n",
       "  1   0.61319    0.16098    3.809  0.000903\n",
       "  2  -0.07305    0.13572   -0.538  0.595594\n",
       "  3   0.32033    0.16852    1.901  0.069925\n",
       "  4   0.08173    0.22148    0.369  0.715480\n",
       "  5   0.03838    0.14700    0.261  0.796334\n",
       "  6  -0.21706    0.17821   -1.218  0.235577,\n",
       "  'Properties':     n  Sigma  df  R_squared  R_squared_a  F_statistic       P_value\n",
       "  0  30  7.068  23     0.7326       0.6628         93.0  5.884182e-15},\n",
       " 'data': {'n': 30,\n",
       "  'k': 6,\n",
       "  'Y': PythonRDD[59] at collect at /tmp/ipykernel_31/2237910587.py:50,\n",
       "  'Y_Predicted': PythonRDD[60] at RDD at PythonRDD.scala:53,\n",
       "  'X': PythonRDD[61] at RDD at PythonRDD.scala:53,\n",
       "  'Residuals': PythonRDD[62] at RDD at PythonRDD.scala:53,\n",
       "  'DataFrame': DataFrame[Y: string, X1: string, X2: string, X3: string, X4: string, X5: string, X6: string],\n",
       "  'Beta': [np.float64(10.787076385734736),\n",
       "   np.float64(0.6131876078096674),\n",
       "   np.float64(-0.07305014309955649),\n",
       "   np.float64(0.32033211637605774),\n",
       "   np.float64(0.08173213353191855),\n",
       "   np.float64(0.03838144730182602),\n",
       "   np.float64(-0.2170566815864987)],\n",
       "  'Cov_Matrix': array([[ 1.34310883e+02, -1.27054423e-01, -1.81523035e-01,\n",
       "          -3.07411947e-01, -2.16292873e-01, -1.13204004e+00,\n",
       "           3.27824332e-02],\n",
       "         [-1.27054423e-01,  2.59155633e-02, -8.17462800e-03,\n",
       "          -8.18457003e-03, -1.85724605e-02,  2.33675656e-05,\n",
       "           1.15391223e-02],\n",
       "         [-1.81523035e-01, -8.17462800e-03,  1.84211915e-02,\n",
       "          -3.12285691e-03,  2.30635793e-03, -4.57418015e-04,\n",
       "          -4.46433998e-03],\n",
       "         [-3.07411947e-01, -8.18457003e-03, -3.12285691e-03,\n",
       "           2.83990978e-02, -7.68988575e-03,  4.84708796e-03,\n",
       "          -1.04279821e-02],\n",
       "         [-2.16292873e-01, -1.85724605e-02,  2.30635793e-03,\n",
       "          -7.68988575e-03,  4.90523616e-02, -9.13072833e-03,\n",
       "          -1.68544107e-02],\n",
       "         [-1.13204004e+00,  2.33675656e-05, -4.57418015e-04,\n",
       "           4.84708796e-03, -9.13072833e-03,  2.16076600e-02,\n",
       "          -3.34960244e-03],\n",
       "         [ 3.27824332e-02,  1.15391223e-02, -4.46433998e-03,\n",
       "          -1.04279821e-02, -1.68544107e-02, -3.34960244e-03,\n",
       "           3.17586156e-02]])}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the following variables are defined:\n",
    "# beta, se, t_werte, p_werte, residuals, quantile, sigma_hat, df_residuals,\n",
    "# r_quadrat, adjusted_r_quadrat, f_statistiken, f_p_wert, n, k, Y, y_pred, X, dataframe, cov_matrix\n",
    "\n",
    "# Step 1: Create the summary dictionary with nested DataFrames\n",
    "result = {\n",
    "    'summary': {\n",
    "        'Coefficients': pd.DataFrame({\n",
    "            'Estimate': np.round(beta, 5),\n",
    "            'Std_Error': np.round(se, 5),\n",
    "            't_value': np.round(t_values, 3),\n",
    "            'p_value': np.round(p_values, 6)\n",
    "        }),\n",
    "        'Properties': pd.DataFrame({\n",
    "            'n': [n],\n",
    "            'Sigma': [np.round(sigma_hat, 4)],\n",
    "            'df': [f_residuals],\n",
    "            'R_squared': [np.round(r_squared, 4)],\n",
    "            'R_squared_a': [np.round(adjusted_r_squared, 4)],\n",
    "            'F_statistic': [np.round(f_statistics, 1)],\n",
    "            'P_value': [f_p_value]\n",
    "        })\n",
    "    },\n",
    "    'data': {\n",
    "        'n': n,\n",
    "        'k': k,\n",
    "        'Y': Y_rdd,\n",
    "        'Y_Predicted': Y_pred_rdd,\n",
    "        'X': X_rdd,\n",
    "        'Residuals': residuals_rdd,\n",
    "        'DataFrame': df,\n",
    "        'Beta': beta,\n",
    "        'Cov_Matrix': cov_matrix\n",
    "    }\n",
    "}\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75edbea9-c9c5-4822-afd4-6b44a501603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbfa7d0-d511-4004-ada3-41ad10ea7035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
